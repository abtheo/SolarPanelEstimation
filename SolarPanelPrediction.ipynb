{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SolarPanelPrediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abtheo/SolarPanelEstimation/blob/master/SolarPanelPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBRcedUVH4Vf",
        "colab_type": "text"
      },
      "source": [
        "#Predicting Solar Panel Production ðŸŒž\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCNj7jv-luwy",
        "colab_type": "text"
      },
      "source": [
        "#Mission Statement ðŸŽ¯\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> ***Wouldn't it be great if could predict the power output of solar panels, without paying to install an on-site meter?***\n",
        "\n",
        "\n",
        "Every solar panel owner would love to know how much power their panels are producing for them. Unfortunately, this knowledge currently comes at the cost of installing a physical electricity meter on-site to read from the panels directly. Fortunately though, every site *already has* an electricity meter installed, which measures the total energy delivered and returned to the national grid. \n",
        "\n",
        "\n",
        "That's where we come in! We use only data from the existing grid meters, combined with freely available meteorological data (courtesy of the [Koninklijk Nederlands Meteorologisch Instituut](https://https://www.knmi.nl)). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHWkdG6BCizV",
        "colab_type": "text"
      },
      "source": [
        "#Setup âš™\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        ">Run this section upon launching the notebook to load all the required dependencies.\n",
        "\n",
        "\n",
        "\n",
        "*Though all code blocks contain explanatory comments, it would be helpful to have a basic understanding of the following libraries:*\n",
        "\n",
        "*   [*Numpy*](https://numpy.org)\n",
        "*   [*Pandas*](https://pandas.pydata.org/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5TUbc2AAhmj",
        "colab_type": "text"
      },
      "source": [
        "##Set File Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSCVf-D_swiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Setting Root Directory:\n",
        "Change root_dir to your relative path to the tutorial folder.\"\"\"\n",
        "!git clone \"{}\" ./temp      # clone github repository to temp folder\n",
        "!mv ./temp/* \"{PROJECT_PATH}\"       # move all files/folders in temp folder to folder defined in project path\n",
        "!rm -rf ./temp                      # remove all the files/folders in temp folder\n",
        "!rsync -aP --exclude=data/ \"{PROJECT_PATH}\"/*  ./   # use remote sync to copy from google drive to local runtime google colab\n",
        "                                                    # but exclude data folder\n",
        "                                                    # https://www.computerhope.com/unix/rsync.htm\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SkRZ_sNEeke",
        "colab_type": "text"
      },
      "source": [
        "##Import Dependencies\n",
        "Python libraries utilised throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjEZYDvAEdIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Standard Python libraries\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "#Data Science / plotting libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import seaborn as sns\n",
        "%pip install ann_visualizer #Drawing package for neural networks, see: https://github.com/Prodicode/ann-visualizer\n",
        "from ann_visualizer.visualize import ann_viz\n",
        "import graphviz\n",
        "\n",
        "#File interactions\n",
        "import pickle\n",
        "import glob\n",
        "\n",
        "#High-level Machine Learning API for TensorFlow.\n",
        "#See: https://keras.io\n",
        "# #%tensorflow_version 2.x\n",
        "#Suppress TF warnings\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\n",
        "import keras\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.optimizers import *\n",
        "from keras import regularizers\n",
        "from keras.callbacks import *\n",
        "\n",
        "#Sci-kit Learn, implements manly helpful tools for ML\n",
        "#See: https://scikit-learn.org/\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import *\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn import tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oplcBbOJOZfM",
        "colab_type": "text"
      },
      "source": [
        "#Supervised Learning - Overview ðŸ‘¨â€ðŸ«\n",
        "\n",
        "---\n",
        "\n",
        "At heart, we face a **supervised learning problem**. This field is a subset of machine learning and AI as a whole, and tackles any problem which can be framed in the following way:\n",
        "\n",
        ">***Given enough examples of some input data, and desired output data; If a relationship exists between input and output, a predictive model can be trained to approximate this relationship.***\n",
        "\n",
        "The \"input data\" we have initially is readings from electricity meters for a given location. Specifically, the total electricity Delivery and Return (in KWh) read through the meter, on an hourly basis. These two input measurements are known collectively as **features**, and are denoted mathemetically as  $x$ .\n",
        "\n",
        "The desired \"output data\" is the electricity production of solar PV panels located at the target site (again in KWh). Known as the **target** or **label**, it is denoted mathematically as $y$ .\n",
        "\n",
        "\n",
        "We work under the assumption that after suitable *feature engineering*, there exists some suitable function $f$ which satisfies the equation:\n",
        "\n",
        ">$f(x) = y$\n",
        "\n",
        "\n",
        "Therefore, learning an approximation of $f$ will allow us to predict the desired targets $y$ when provided with unseen examples of input data $x$. \n",
        "\n",
        "A **predictive model** does precisely this; it is a specialised implementation of $f(x)$, fine-tuned to your specific problem domain to give realistic predictions of the targets $y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCyrbk_PzuDE",
        "colab_type": "text"
      },
      "source": [
        "##Importing Raw Data\n",
        "Let's take a look at an example dataset of our features and targets. This data comes from Havelte, at a solar production site.\n",
        "\n",
        "\n",
        "\n",
        "*   Features ($x$) are Delivery and Return.\n",
        "*   Target ($y$) is Back Production. (BPM)\n",
        "*   Date and Timestamp are when the measurement was made.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09jr2oQYdfWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Section 1 uses the raw readings subdirectory, so set to the current data_dir.\n",
        "data_dir = root_dir + \"Raw_Readings/\"\n",
        "\n",
        "#Read data from an example CSV file as a Pandas DataFrame.\n",
        "dataset = pd.read_csv(data_dir+\"EnergyAndPV_0.csv\", sep=';', decimal=',') \n",
        "\n",
        "#Take a look at a few rows of data\n",
        "dataset.iloc[10:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPpProgIz5aS",
        "colab_type": "text"
      },
      "source": [
        "##Data Visualisation\n",
        "It's helpful to start by plotting our data to get a better idea of what we're working with. In particular, we're looking for relationships between the features and the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKy4xLi2nNO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Plot features and targets against timestep iterations on the same axes.\"\"\"\n",
        "X = list(range(len(dataset)))\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "#Plot BackProduction\n",
        "fig.add_trace(go.Scatter(\n",
        "                x=X,\n",
        "                y=dataset['BackProduction'],\n",
        "                name='Solar PV Production',\n",
        "                line_color='mediumseagreen',\n",
        "                opacity=0.8))\n",
        "\n",
        "#Plot Return\n",
        "fig.add_trace(go.Scatter(\n",
        "                x=X,\n",
        "                y=dataset['Return'],\n",
        "                name='Electricity Returned',\n",
        "                line_color='red',\n",
        "                opacity=0.8))\n",
        "\n",
        "#Plot Delivery\n",
        "fig.add_trace(go.Scatter(\n",
        "                x=X,\n",
        "                y=dataset['Delivery'],\n",
        "                name='Delivered Electricity',\n",
        "                line_color='royalblue',\n",
        "                opacity=0.8))\n",
        "\n",
        "#Make it fancy!\n",
        "fig.update_layout(go.Layout(\n",
        "    title=dict(x=0.45),\n",
        "    yaxis=dict(title='Energy (KWh)'),\n",
        "    xaxis=dict(title='Timestep'),\n",
        "    title_text=\"Havelte - Back Production vs Return\",\n",
        "    hovermode=\"x\",\n",
        "    template=\"plotly_dark\"))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcT61cj1bVYs",
        "colab_type": "text"
      },
      "source": [
        "##Correlations & Heatmap\n",
        "Checking correlations and their coefficients is also a useful metric.\n",
        "If a feature has a high correlation with the target, it *should* be a useful feature.\n",
        "\n",
        "However, features should not *necessarily* be discarded\n",
        "just because of a low correlation in 1 dimension,\n",
        "as they could potentially be useful when combined in higher dimensional space.\n",
        "\n",
        "\n",
        "*Recommended Reading: [Correlation Coefficients](https://blog.bigml.com/2015/09/21/looking-for-connections-in-your-data-correlation-coefficients/)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGpmD0_qZ6Yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Pearson Correlation Heatmap\n",
        "â€” Closer to 0 implies weaker correlation (exact 0 implying no correlation)\n",
        "â€” Closer to 1 implies stronger positive correlation\n",
        "â€” Closer to -1 implies stronger negative correlation\"\"\"\n",
        "cmap = plt.cm.coolwarm\n",
        "sns.heatmap(dataset.corr(), annot=True, cmap=cmap,  fmt='g')\n",
        "\n",
        "#Print correlation coefficients numerically\n",
        "dataset.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ8QYM9GZiRz",
        "colab_type": "text"
      },
      "source": [
        "Furthermore, we can create a [pair-plot](https://seaborn.pydata.org/generated/seaborn.pairplot.html), whereby each column of data is plotted against one another. Once again, we're especially interested in linear correlations between the targets and features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxsb_uSAZi51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Plot features against one another in a pair grid. \"\"\"\n",
        "#Using Seaborn for visualisation. For details, see: https://seaborn.pydata.org/\n",
        "g = sns.PairGrid(dataset)\n",
        "g.map(plt.scatter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za1tIUUImlRo",
        "colab_type": "text"
      },
      "source": [
        "##Comparing Multiple Datasets\n",
        "Havelte looks promising! We can see that the feature 'Return' has an extremely strong correlation to the target, 'BackProduction'. This is good evidence that a relationship exists between our features and targets, so the equation $f(x) â‰ˆ y$ can be satisfied.\n",
        "\n",
        "\n",
        "However, not all datasets are so easy! Havelte is a solar production plant, meaning that the power returned to the grid maps very closely to the total production in this site. Let's see if this holds true across different sites, by taking a look at the Return vs BackProduction for all of our available datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZGeX4h66VVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Iterate over all the dataset CSV files, plotting Return (in red) vs BackProduction (in green).\n",
        "Takes a minute!\"\"\"\n",
        "filepaths = glob.glob(data_dir+\"*.csv\")\n",
        "dataset_list = []\n",
        "\n",
        "for i,f in enumerate(filepaths):\n",
        "  #Read current CSV into a Pandas DataFrame\n",
        "  dataset = pd.read_csv(f, sep=';', decimal=',')\n",
        "\n",
        "  #Add dataset (with path) to superset, so we only have to load them once.\n",
        "  dataset_list.append([f, dataset])\n",
        "\n",
        "  #variables for graphs\n",
        "  name = f[len(data_dir):-4]\n",
        "  X = list(range(len(dataset)))\n",
        "  #Add plot of Return vs BackDelivery of the current dataset\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter(\n",
        "                  x=X,\n",
        "                  y=dataset['BackProduction'],\n",
        "                  name=name + \"Back Production\",\n",
        "                  line_color='mediumseagreen',\n",
        "                  opacity=0.8))\n",
        "\n",
        "  fig.add_trace(go.Scatter(\n",
        "                  x=X,\n",
        "                  y=dataset['Return'],\n",
        "                  name=\"Return\",\n",
        "                  line_color='red',\n",
        "                  opacity=0.8))\n",
        "  \n",
        "\n",
        "  #Make it fancy!\n",
        "  fig.update_layout(go.Layout(\n",
        "      title=dict(x=0.45),\n",
        "      yaxis=dict(title='Energy (KWh)'),\n",
        "      xaxis=dict(title='Timestep'),\n",
        "      title_text=f\"{name} - Back Production vs Return\",\n",
        "      hovermode=\"x\",\n",
        "      template=\"plotly_dark\"))\n",
        "  fig.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dy2pL5PdM4O",
        "colab_type": "text"
      },
      "source": [
        "Uh-oh! Looks like Havelte isn't exactly a typical site; the Back Production of most sites is definitely correlated with their Return, but not as cleanly as in a pure solar production plant. Not to worry - the relationship still exists, we're just gonna have to get smarter with our feature data, as we'll see in the 'Feature Engineering' section below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfNGcx8Ex0av",
        "colab_type": "text"
      },
      "source": [
        "## Defining Scope & Limitations\n",
        "We can also see that some of these sites lack any obvious $x \\rightarrow y$ relationship. \n",
        ">In particular, Haarlem, Stiens, and both the Amsterdam sites are quite obviously unusable.\n",
        "\n",
        "The real world cause of this is that the site is usually consuming 100% of the solar panel production in-house, rather than sending any back to the grid for us to measure. In these cases, Back Production is simply not predictable, and we will discontinue working with such sites.\n",
        "\n",
        "\n",
        "It is important to be able to distinguish the trash from the treasure mathematically, rather than visual intuition. Furthermore, we must do so based on purely the features $x$, as we *will not* have the target $y$ when attempting to predict unseen sites.\n",
        "\n",
        "\n",
        "The following code snippet works on the assumption that \"trash\" sites are outliers with respect to their inter-feature covariance.\n",
        "\n",
        ">**Important Note:** Only *features* can be considered when determining whether a site will be predictable. The target column *should not* be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu7_ZFDfd-fJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Taking out the trash:\n",
        "If the covariance coefficient between Delivery and Return exceeds a certain predefined threshold,\n",
        "exlclude the dataset from further use.\"\"\"\n",
        "#Iterate over datasets\n",
        "for path, dataset in dataset_list:\n",
        "  #MinMaxScaler class - Transforms each column's values to lie within a given range.\n",
        "  #This puts each independent dataset on 'equal footing' for calculating correlation.\n",
        "  #Further detail on scaling is available in the Machine Learning section below, or at:\n",
        "  #https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
        "  minmax_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "  features = [\"Delivery\", \"Return\"]\n",
        "  scaled_dataset = minmax_scaler.fit_transform(dataset[features])\n",
        "\n",
        "  #Divide-by-zero errors cause NaNs in Numpy, so trim zero value points.\n",
        "  el_et = np.array([a for a in scaled_dataset if a[0] > 0 and a[1] > 0])\n",
        "  \n",
        "  #Find the 'magic number'\n",
        "  covar = np.cov(el_et[:,0], el_et[:,1])[1,1]\n",
        "  #Eliminate above threshold\n",
        "  if covar > 0.0441:\n",
        "    print(\"Site: {0};  Covariance = {1}\".format(path[len(root_dir):-4], covar))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hMs2YJPhc9s",
        "colab_type": "text"
      },
      "source": [
        "#Feature Engineering ðŸŒ¤\n",
        "\n",
        "---\n",
        "\n",
        "> *The process of selecting, enriching and optimising features based on their usefulness to a predictive model.*\n",
        "\n",
        "Through the preliminary investigation conducted in the previous section, we have determined that Delivery and Return are useful features for predicting the Back Production of sites within our scope. However, different sites have a wide range of energy profiles, making it difficult to produce a predictive model which can generalise to all cases using only these two features.\n",
        "\n",
        "Luckily, we have domain knowledge! We know that in reality, **solar panel production is dependent upon the weather**. Primarily solar intensity, but a multitude of other factors could potentially be influential. In this section, we enrich site data with weather data from KNMI, and emprically veryify the usefulness of these new features.\n",
        "\n",
        "\n",
        " *Recommended Reading:* [*An Introduction to Variable and Feature Selection*](http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIfpPQfh1wV4",
        "colab_type": "text"
      },
      "source": [
        "##Feature Enrichment - KNMI Weather Data\n",
        "\n",
        "In this section, a new dataset is introduced. Data from the site named '171550' (as seen above) has been merged with climatology data, which has been collected from the geographically closest KNMI weather station. The code for parsing these two data sources has been omitted (it's not at all pretty), but sufficed to say;\n",
        "\n",
        "> ***Each time step in the dataset now has 20 additional weather features!***\n",
        "\n",
        "*For a full explanation of each of these features, see the official [KNMI documentation](https://projects.knmi.nl/klimatologie/uurgegevens/selectie.cgi).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1wi2vi7a0LY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set file directory for data\n",
        "data_dir = root_dir + \"Feature_Engineering/\"\n",
        "path = data_dir + \"0_KNMI_merged.npy\"\n",
        "\n",
        "column_labels = [\"HH\",\"DD\",\"FH\",\"FF\",\"FX\",\"T\",\"TD\",\"SQ\",\"Q\",\"DR\",\"RH\",\"P\",\"VV\",\"N\",\"U\",\"IX\",\"M\",\"R\",\"S\",\"O\",\"Y\",\"EL\",\"ET\",\"BPM\"]\n",
        "\n",
        "dataset = np.load(path, allow_pickle=True)\n",
        "dataframe = pd.DataFrame(dataset, columns=column_labels)\n",
        "\n",
        "#Split into features (X) and targets (Y)\n",
        "X = dataset[:,:-1]\n",
        "Y = dataset[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV6bSfq0YJAl",
        "colab_type": "text"
      },
      "source": [
        "##Daily Aggregation\n",
        "The business use case for this project is to predict solar production on a daily basis. KNMI also only updates their public data once per day, which puts a hard limit on response time. ***Therefore, the datasets have been aggregated from hourly into daily timesteps.***\n",
        "\n",
        "\n",
        "> *Sadly, this means we have to eliminate the 'HH' feature! Although potentially useful, there is no logical way to encode the hour of the day into a daily timestep.*\n",
        "\n",
        "\n",
        "On the bright side, aggregating in this way substantially reduces the noise and variability inherent to the data. This will allow our ML algorithm to converge faster and (hopefully) predict more accurately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0A1LF3Rpx1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove HH column\n",
        "X = dataset[:,1:-1]\n",
        "Y = dataset[:,-1]\n",
        "\n",
        "dataframe = dataframe.drop(columns=[\"HH\"], axis=1)\n",
        "\n",
        "#Adjust number of features\n",
        "n_features = X.shape[1]\n",
        "\n",
        "#Remove HH and BPM from column labels (for plotting)\n",
        "feature_labels = column_labels[1:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DfQhkf3S9-U",
        "colab_type": "text"
      },
      "source": [
        "##Correlations & Heatmaps\n",
        "Just like with our original dataset, let's investigate feature - target correlations, and plot some visualisations to gain better understanding of the new features we're working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVbbvzUUE8Va",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correlations = dataframe.corr()[\"BPM\"]\n",
        "\"\"\"Show correlation coefficients of features to the target BPM.\n",
        "Remember that positive and negative correlations are equally useful for predictions!\n",
        "Therefore, sorting by absolute value is more informative.\"\"\"\n",
        "abs_sorted_correlations = correlations[correlations.abs().sort_values().keys()][::-1]\n",
        "print(abs_sorted_correlations)\n",
        "\n",
        "#Pearson Correlation Heatmaps\n",
        "ax = sns.heatmap(correlations.sort_values().to_frame()[::-1] , xticklabels=True, yticklabels=True, cmap=plt.cm.plasma)\n",
        "plt.show()\n",
        "ax = sns.heatmap(dataframe.corr(), xticklabels=True, yticklabels=True, cmap=plt.cm.plasma)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfbP6CnDZ7gk",
        "colab_type": "text"
      },
      "source": [
        "##Extra Trees Feature Importance\n",
        "A fantastic method of measuring how good your features will be for prediction, is *by using them for prediction*. It is then possible to evaluate which features contributed most to the correct predictions.\n",
        "\n",
        "\n",
        ">*This is an example of **embedded** feature selection - using a machine learning algorithm as part of the feature selection process.*\n",
        "\n",
        "*Further Reading:*\n",
        "*   [*ExtraTrees Feature Selection*](https://www.geeksforgeeks.org/ml-extra-tree-classifier-for-feature-selection/)\n",
        "*   [*LASSO Feature Selection*](https://beta.vu.nl/nl/Images/werkstuk-fonti_tcm235-836234.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP_jB85KXa_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Take a small subset of the data to keep the tree complexity low\n",
        "sample = np.array(random.sample(list(dataset), 10))\n",
        "\n",
        "#Train / Fit model\n",
        "clf = tree.DecisionTreeRegressor()\n",
        "clf = clf.fit(X = sample[:, :-1], y = sample[:, -1:])\n",
        "\n",
        "#Plot\n",
        "d = tree.plot_tree(clf, feature_names=column_labels[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3wMzth9DMBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building the model\n",
        "#See: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html\n",
        "extra_tree_forest = ExtraTreesRegressor(n_estimators=100, n_jobs=-1, \n",
        "                                        max_features=3) #Play with me! Maximum number of features to look simultaneously at per decision node.\n",
        "# Training the model \n",
        "extra_tree_forest.fit(X, Y) \n",
        "\n",
        "#Extract Feature Importances\n",
        "importances = extra_tree_forest.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in extra_tree_forest.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)[::-1][:]\n",
        "\n",
        "labels = list()\n",
        "for i in indices:\n",
        "  labels.append(feature_labels[i])\n",
        "\n",
        "#Plot importances\n",
        "fig = px.bar(dataframe, x=labels, y=importances[indices],\n",
        "             title=\"Feature Importance\",\n",
        "             color=importances[indices],\n",
        "             color_continuous_scale=px.colors.sequential.Agsunset_r,\n",
        "             )\n",
        "#Make it fancy!\n",
        "fig.update_layout(go.Layout(\n",
        "    title=dict(x=0.55),\n",
        "    xaxis=dict(title='Feature'),\n",
        "    yaxis=dict(title='% Importance'),\n",
        "    template=\"plotly_dark\"))\n",
        "fig.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0agw71JY5XaX",
        "colab_type": "text"
      },
      "source": [
        "##Feature Reduction\n",
        "Using the intuition gained from our data interrogation techniques, we have a clear ranking for the importance of each feature. Some of the features are providing almost zero predictive weighting, and so can be discarded with no drawbacks.\n",
        "\n",
        "\n",
        "Real world data concerns mean we should actively aim to eliminate useless features, as data itself has a cost in terms of runtime, storage space, and processing power. For this reason, the following features have been deemed above the threshold of importance, and will henceforth be the only ones included in code.\n",
        "\n",
        "*  ***Q :***  *Global solar radiation (J / cm2)*\n",
        "*  ***SQ :***  *Duration of sunshine (0.1 hours)*\n",
        "*  ***U :***  *Relative humidity (percentage)*\n",
        "*  ***T :***  *Temperature (0.1 degrees Celsius)*\n",
        "*  ***EL :***  *Elektriciteit Levering (KWh), a.k.a Delivery*\n",
        "*  ***ET :***  *Elektriciteit Teruglevering (KWh), a.k.a. Return*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhCxnGUE-6e2",
        "colab_type": "text"
      },
      "source": [
        "#Machine Learning ðŸ¤–ðŸ§ \n",
        "\n",
        "---\n",
        "\n",
        ">*Using algorithms to perform specific tasks without using explicit instructions.*\n",
        "\n",
        "The fun bit! Now that we all of our features decided and our data *almost* completely prepared, it's time to start working with machine learning models. All the ML models discussed here implement two important functions:\n",
        "\n",
        "*  ***Fit :***  *Execute the algorithm on a specific dataset, so that it may attempt to learn the relationship $f(x) = y$ present in the data.*\n",
        "*  ***Predict :***  *Use the relationship learned during fitting to produce a predicted value of $y$ for any given $x$.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcLrNmdUBgk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Set data directory for ML section.\"\"\"\n",
        "data_dir = root_dir + \"Training_Data/\"\n",
        "\n",
        "filepaths = glob.glob(data_dir + \"*.npy\")\n",
        "\n",
        "\"\"\"Declare feature shape. In our case;\n",
        "#[Q, SQ, U, T, EL, ET] => [BPM]\"\"\"\n",
        "n_features = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83yhl7pu_aH1",
        "colab_type": "text"
      },
      "source": [
        "##Train : Test : Validation Splitting\n",
        "In order for our machine learning model to actually *learn*, we need to provide many correct examples of what we want it to do. The ML algorithm will iterate over these example datapoints (in [batches](https://medium.com/analytics-vidhya/when-and-why-are-batches-used-in-machine-learning-acda4eb00763)), make a prediction, and then compare the prediction to the actual target. By evaluating the error between these two vectors, the model can update itself to try and correct for these errors, and thus learns to make more accurate predictions.\n",
        "\n",
        "\n",
        "However, when it comes to evaluation, *we cannot evaluate with data points used to train the model!* As the algorithm has already seen the actual target for these points, it has learnt the correct answer. This would give the false impression that our model performs extremely well, but that performance would fall apart when presented with new unseen data. In other words, that's cheating!\n",
        "\n",
        "\n",
        "To avoid this [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/) problem, we split the data into three distinct groups:\n",
        "\n",
        "* ***Training Set:*** *Used to train the ML model. (80% of dataset)*\n",
        "\n",
        "* ***Testing Set:*** *Used to evaluate the performance of the ML model. Can be used during training to help optimise learning. (20% of dataset)*\n",
        "\n",
        "* ***Validation Set:*** *Used for the final model evaluation. Must remain unseen by the model until all weights are fixed. (Complete data from one site is left out of training entirely, aka out-of-sample data)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkpI95avBAgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"This practise is simple to implement in code using list slicing, as shown here with an example Numpy matrix. \n",
        "This snippet is used as part of the 'split_scale_datasets' function described below.\"\"\"\n",
        "\n",
        "example_dataset = np.ones(shape=(1000, 7))\n",
        "#Randomly shuffle\n",
        "np.random.shuffle(example_dataset)\n",
        "\n",
        "#Find indexes by dividing length\n",
        "total_length = len(example_dataset)\n",
        "train_index = math.floor(total_length * 0.8)\n",
        "\n",
        "#Split into sets\n",
        "train_set = example_dataset[:train_index]\n",
        "test_set = example_dataset[train_index:]\n",
        "\n",
        "print(f\"Train set shape: {train_set.shape} \\nTest set shape: {test_set.shape}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DcYgHsXC-CZ",
        "colab_type": "text"
      },
      "source": [
        "##Scaling & Normalization\n",
        "\n",
        "For most ML algorithms, this step is *critical*, for two main reasons. Firstly, to give features equal weighting in the training process. Secondly, it mitigates the [exploding / vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).\n",
        "\n",
        "This is easily solved with MinMax Scaling and Standard Scaling:\n",
        "> ***MinMax***:  $X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}}$\n",
        "\n",
        "> ***Standardize***:  $X_{z} = X - \\overline{X} \\over \\sigma_{X}$\n",
        "\n",
        "\n",
        "Scaling is *even more* important for our specific use case. Each site has a different energy profile, and so will ultimately have a variable scale coefficient. This will cause the model to learn converge towards the average scale of all datasets when taken together, *which causes predictions on individual sites to suffer* ***drastically!*** Formulaicly, this means for our problem:\n",
        "\n",
        ">  $f(x) \\propto y$\n",
        "\n",
        "or,\n",
        "> $f(x) = BPM * c$, where c is different for each real-world site.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSC8_bu_5ARP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**REDACTED**\n",
        "\n",
        "\n",
        "---\n",
        "The following code has been altered to obscure potentially proprietary information. The exact method of scaling has been removed from this public repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ql6al4b_iK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Class for implementing the aforementioned 'magic scaling' equation.\n",
        "Inherits from Sklearn's Scaler base class.\"\"\"\n",
        "class MagicScaler(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self):\n",
        "    self.magic_scale = 1\n",
        "\n",
        "  #Determine magic scale\n",
        "  def fit(self, X):\n",
        "    \"\"\"\n",
        "    <REDACTED>\n",
        "    Magic scale equation is proprietary information.\n",
        "    It has been redacted from the public release.\n",
        "    </REDACTED>\n",
        "    \"\"\"\n",
        "    magic = 0.5\n",
        "    self.magic_scale = 1 / magic\n",
        "    return magic\n",
        "\n",
        "  def transform(self, data):\n",
        "    return data * self.magic_scale\n",
        "\n",
        "  def fit_transform(self, X, data):\n",
        "    \"\"\"fits magic scale to X,\n",
        "    then scales and returns data\"\"\"\n",
        "    self.fit(X)\n",
        "    return self.transform(data)\n",
        "\n",
        "  def inverse_transform(self, data):\n",
        "    return data / self.magic_scale\n",
        "\n",
        "\"\"\"Function to load data from all provided filepaths,\n",
        "concatenate them into a single dataset,\n",
        "and perform appropriate scaling operations.\n",
        "\n",
        "Returned data is ready to go straight into an ML model!\"\"\"\n",
        "def split_scale_datasets(filepaths, train_split=0.8):\n",
        "  print(\"Reading datasets from the following files:\", filepaths)\n",
        "  #Initialize arrays and explicitly-shaped empty tensors for concatenation\n",
        "  X = np.empty(shape=(1, n_features))\n",
        "  Y = np.empty(shape=(1))\n",
        "  Xs = np.array([])\n",
        "  Ys = np.array([])\n",
        "\n",
        "  #Scale all features by their column's mean and std, then between range [0-1]\n",
        "  std_scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "  minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  \n",
        "  #instantiate magic scaler implementation (discussed above)\n",
        "  Y_magic_scaler = MagicScaler()\n",
        "\n",
        "  #Agglomerate all datasets into single ND-array (Sorry for the mess! <3)\n",
        "  for F in filepaths:\n",
        "    f = np.load(F)\n",
        "    if not (len(Xs) == 0):\n",
        "      Xs = np.concatenate( (Xs,  minmax_scaler.fit_transform(std_scaler.fit_transform(f[:,:-1]))), axis=0) #Split X from dataset, scale, and append to master set\n",
        "      Ys = np.concatenate( (Ys,  Y_magic_scaler.fit_transform(f[:,:-1], f[:,-1].reshape(-1,1))), axis=0) #Split Y from dataset, scale, and append to master list\n",
        "    #Please ignore! - gross extra condition needed for the first iteration only.\n",
        "    else:\n",
        "      Xs = np.array(  minmax_scaler.fit_transform(std_scaler.fit_transform(f[:,:-1])))\n",
        "      Ys = np.array( Y_magic_scaler.fit_transform(f[:,:-1], f[:,-1].reshape(-1,1)))\n",
        "  print(f\"Features shape: {Xs.shape};  Target shape:{Ys.shape}\")\n",
        "\n",
        "  #Find indexes by dividing length\n",
        "  total_length = len(Xs)\n",
        "  train_index = math.floor(total_length * train_split)\n",
        "\n",
        "  #Split into train : test : val\n",
        "  train_X = Xs[:train_index]\n",
        "  train_Y = Ys[:train_index]\n",
        "\n",
        "  test_X = Xs[train_index:]\n",
        "  test_Y = Ys[train_index:]\n",
        "\n",
        "  print(f\"Train size: {train_Y.shape[0]};  Test size: {test_Y.shape[0]}\")\n",
        "\n",
        "  return train_X, train_Y, test_X, test_Y\n",
        "  \n",
        "#Call function to get prepared datasets\n",
        "train_X, train_Y, test_X, test_Y = split_scale_datasets(filepaths, train_split=0.91)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9odRn5rBEkGQ"
      },
      "source": [
        "##Artificial Neural Networks\n",
        "Finally, we arrive at the actual solution used to achieve the mission statement; Artificial Neural Networks. This is a whole beast of a topic in its own right, so this notebook will only provide a rudimentary overview for brevity. There are however some key terms which cannot be done without:\n",
        "\n",
        "*   ***Neuron:*** *Single unit of computation. Takes multiple inputs, does math to them, and produces a single output via an [activation function](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html).*\n",
        "\n",
        "*   ***Layer:*** *A collection of interconnected neurons. Takes a [tensor](https://www.tensorflow.org/guide/tensor) as an input and returns a new tensor as an output, potentially flowing into. (hence the name, TensorFlow!)*\n",
        "\n",
        "*  ***Hyperparameter:*** *Variable factors when constructing and training the model, such as number of neurons in a layer. Think of them like 'settings'.*\n",
        "\n",
        "*   ***Loss:*** *a measure of error between the predictions a model produces and the actual target values. In our case, [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) is the chosen metric. (lower = better)*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "To learn more, here's a more in-depth [introductory article](https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9) (but there are tons of resources out there!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zezqB6HiX_gc",
        "colab_type": "text"
      },
      "source": [
        "###Building a Brain\n",
        "Well, defining the architecture of a model and constructing it in code, anyway. At an abstract level, you can interpreit the arrangement of layers and neurons as the way the model will \"think\". More complex models are needed to tackle more complex problems, but incurr drawbacks such as higher data cost and [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
        "\n",
        "\n",
        "Here we define about as simple a network as possible for tackling this task.\n",
        "\n",
        "* ***Input layer(n=6):***  Same number of neurons as our number of features.\n",
        "* **[*Dense*](https://keras.io/layers/core/) *hidden layer(n=256)*** :  A fully connected layer. Number of neurons is arbitrarily chosen for now. This is discussed in detail later in the Hyperparameter Optimisation section.\n",
        "* Output layer(n=1):  Our target is a scalar value, so only one output neuron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aTMTz1tNiXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using Keras, see: https://keras.io\n",
        "#Returns a model with a compiled architecture and empty weights.\n",
        "def build_nn_model(n_features):\n",
        "  model = Sequential()\n",
        "  #Input layer:\n",
        "  #n_feature neurons -> Hidden Layer of 256 neurons\n",
        "  model.add(Dense(256, input_shape=((n_features,)), activation='relu'))\n",
        "  #Output layer:\n",
        "  #256 Hidden -> 1 Output neuron (the shape of our target, i.e. a scalar value)\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  model.compile(optimizer=Adam(lr=1e-3, decay=1e-4),\n",
        "                loss=['mean_squared_error'])\n",
        "  return model\n",
        "\n",
        "#Construct model\n",
        "model = build_nn_model(n_features)\n",
        "\n",
        "#Print a summary of the model architecture\n",
        "model.summary()\n",
        "\n",
        "#Draw a pretty network graph!\n",
        "graph_fname = 'network_diagram.gv'\n",
        "ann_viz(model, title=\"Simple Model Brain\", filename=graph_fname)\n",
        "with open(graph_fname) as f:\n",
        "    dot_graph = f.read()\n",
        "graphviz.Source(dot_graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lI89bxWJ1l0",
        "colab_type": "text"
      },
      "source": [
        "###Training a Model\n",
        "Now that we have constructed a basic model, it's time to train it on our pre-processed datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV7Y8BctfSiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Callbacks are functions that run on every training epoch. See: https://keras.io/callbacks/\n",
        "The Checkpoint callback calculates the model's loss for predictions on the testing set.\n",
        "The best model's weights are saved to be loaded after training is complete, \n",
        "as the final training epoch is not necissarily the most optimal model.\"\"\"\n",
        "simple_brain_path = \"/simple_solar_brain.h5\"\n",
        "checkpoint_callback = ModelCheckpoint(filepath=simple_brain_path, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "n_epochs = 100\n",
        "\n",
        "#Start training!\n",
        "history = model.fit(x=train_X,\n",
        "          y=train_Y,\n",
        "          batch_size=64,\n",
        "          validation_data=(test_X, test_Y),\n",
        "          verbose=1,\n",
        "          callbacks=[checkpoint_callback],\n",
        "          epochs=n_epochs)\n",
        "#Reload best model\n",
        "model.load_weights(simple_brain_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaB9mq9-qKJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Visualise training history:\"\"\"\n",
        "#Plot Losses\n",
        "fig_acc = go.Figure()\n",
        "fig_acc.add_trace(go.Scatter(\n",
        "                x=list(range(n_epochs)),\n",
        "                y=history.history['loss'],\n",
        "                name='Train Loss',\n",
        "                line_color='red',\n",
        "                opacity=0.8))\n",
        "\n",
        "fig_acc.add_trace(go.Scatter(\n",
        "                x=list(range(n_epochs)),\n",
        "                y=history.history['val_loss'],\n",
        "                name='Test Loss',\n",
        "                line_color='green',\n",
        "                opacity=0.8))\n",
        "#Make it fancy!\n",
        "fig_acc.update_layout(go.Layout(\n",
        "    title=dict(x=0.45),\n",
        "    yaxis=dict(title='Loss'),\n",
        "    xaxis=dict(title='Epoch'),\n",
        "    title_text=\"Training History - Loss\",\n",
        "    hovermode=\"x\",\n",
        "    template=\"plotly_dark\"))\n",
        "fig_acc.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ2cOzNTI5R6",
        "colab_type": "text"
      },
      "source": [
        "###Making Predictions\n",
        "> *Hooray, we have a trained ML model! Now to put it to use.*\n",
        "\n",
        "\n",
        "However, you'll probably notice that the training graphs are... terrible. At the very least, the training loss should decay expontentially, telling us that the model was able to learn for at least a couple epochs.\n",
        "\n",
        "\n",
        "Amazingly though, this simple model can still perform admirably when predicting on our validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goOpBsf-LUnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load validation file\n",
        "validation = np.load(root_dir + \"Validation_Data/Emmeloord.npy\")\n",
        "\n",
        "#Split\n",
        "val_X = validation[:,:-1]\n",
        "val_Y = validation[:,-1]\n",
        "\n",
        "#Scale all features by their column's mean and std, then between range [0-1]\n",
        "Y_magic_scaler = MagicScaler()\n",
        "Y_magic_scaler.fit(val_X)\n",
        "\n",
        "std_scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "val_X = minmax_scaler.fit_transform(std_scaler.fit_transform(val_X))\n",
        "\n",
        "raw_predictions = model.predict(val_X)\n",
        "\n",
        "#Unscale to real values\n",
        "predictions = Y_magic_scaler.inverse_transform(raw_predictions)\n",
        "\n",
        "#Plot Predictions vs Actual\n",
        "fig_1 = go.Figure()\n",
        "\n",
        "fig_1.add_trace(go.Scatter(\n",
        "                x=list(range(len(val_Y))),\n",
        "                y=list(val_Y),\n",
        "                name='Actual',\n",
        "                line_color='green',\n",
        "                opacity=0.8))\n",
        "\n",
        "fig_1.add_trace(go.Scatter(\n",
        "                x=list(range(len(val_Y))),\n",
        "                y=list(predictions.ravel()),\n",
        "                name='Predicted',\n",
        "                line_color='red',\n",
        "                opacity=0.8))\n",
        "\n",
        "#Make it fancy!\n",
        "fig_1.update_layout(go.Layout(\n",
        "    title=dict(x=0.45),\n",
        "    yaxis=dict(title='Energy (KWh)'),\n",
        "    xaxis=dict(title='Timestep'),\n",
        "    title_text=\"Predictions vs. Actual\",\n",
        "    hovermode=\"x\",\n",
        "    template=\"plotly_dark\"))\n",
        "\n",
        "fig_1.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPK7coTveXhs",
        "colab_type": "text"
      },
      "source": [
        "###Convolutional Neural Networks (CNNs)\n",
        "These are a specialised implementation of neural networks, which pay explicit attention to the spatial ordering of the input features. The typically use-case for CNNs is in the domain of image recognition, as the order of pixels within the image is fundamentally important. \n",
        "\n",
        "*Further Reading: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53*\n",
        "\n",
        "In this case, we utilise this spatial information by transforming the input data into batches of a certain number of timesteps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbiPNqPGpNZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Batch split train / test datasets.\n",
        "Takes a 14 day timeslice; to predict today, 13 days of previous data are needed.\"\"\"\n",
        "batch_size = 14\n",
        "#Split Train into Batches\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(batch_size, train_X.shape[0]):# - diff\n",
        "    X.append(train_X[i-batch_size:i, :n_features])\n",
        "    Y.append(train_Y[i])\n",
        "train_X_batches, train_Y_batches = np.array(X), np.array(Y)\n",
        "\n",
        "#Split Test into Batches\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(batch_size, test_X.shape[0]):\n",
        "    X.append(test_X[i-batch_size:i, :n_features])\n",
        "    Y.append(test_Y[i])\n",
        "test_X_batches, test_Y_batches = np.array(X), np.array(Y)\n",
        "\n",
        "print(test_X_batches.shape, test_Y_batches.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuN3z6aq_RHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_solar_deep_cnn(features, batch_size=14):\n",
        "  input_layer = Input([batch_size, features])\n",
        "\n",
        "  #ConvNet Timeslice approach\n",
        "  x = Conv1D(256, 3, padding='same', activation='relu')(input_layer)\n",
        "  x = Conv1D(256, 3, padding='same', activation='relu')(x)\n",
        "  x = Conv1D(256, 3, padding='same', activation='relu')(x)\n",
        "  x = MaxPooling1D(pool_size=2, padding='same')(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "\n",
        "  #Skip Connection\n",
        "  skip = Flatten()(x)\n",
        "  skip = Dense(256, activation='relu', kernel_regularizer=regularizers.l1_l2(0.001,0.001))(skip)\n",
        "\n",
        "  #ConvBlock2\n",
        "  x = Conv1D(128, 3, padding='same', activation='relu')(x)\n",
        "  x = Conv1D(128, 3, padding='same', activation='relu')(x)\n",
        "  x = MaxPooling1D(pool_size=2, padding='same')(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "\n",
        "  #ConvBlock3\n",
        "  x = Conv1D(64, 3, padding='same', activation='relu')(x)\n",
        "  x = MaxPooling1D(pool_size=2, padding='same')(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "\n",
        "  #Conv -> Dense\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(256, activation='relu', kernel_regularizer=regularizers.l1_l2(0.001,0.001))(x)\n",
        "\n",
        "  #Merge skip layer\n",
        "  merged = Add()([x,skip])\n",
        "  x = Dense(512, activation='relu', kernel_regularizer=regularizers.l1_l2(0.001,0.001))(merged)\n",
        "  x = Dropout(0.4)(x)\n",
        "\n",
        "  output_layer = Dense(1)(x)\n",
        "\n",
        "  model = Model(inputs = input_layer, outputs = output_layer)\n",
        "\n",
        "  model.compile(optimizer=Adam(lr=0.0001, decay=1e-4),\n",
        "                loss=['mean_squared_error'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "solar_model = None\n",
        "#Construct solar and print a network summary\n",
        "solar_model = build_solar_deep_cnn(n_features)\n",
        "solar_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKVbVFgcM6zh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Callbacks - Run every each training epoch\"\"\"\n",
        "#TensorBoard logging -- Pretty visualisations for models & training progress\n",
        "logdir = \"/content/tensor_logs/\"\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "#Reduces learning rate when the model ceases to improve over multiple epochs to really squeeze out an optimised solution\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.9,\n",
        "                            patience=5, min_lr=1e-5)\n",
        "solar_brain_dir = \"/content/solar_brain.h5\"\n",
        "#Save best model from training\n",
        "checkpoint_callback = ModelCheckpoint(solar_brain_dir, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "#Only stop training once the model has completely converged\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=100, min_delta=1e-4)\n",
        "n_epochs = 5000\n",
        "\n",
        "\"\"\"Start training!\"\"\"\n",
        "history = solar_model.fit(x=train_X_batches,\n",
        "          y=train_Y_batches,  \n",
        "          batch_size=256,\n",
        "          validation_data=(test_X_batches, test_Y_batches),\n",
        "          verbose=1,\n",
        "          callbacks=[checkpoint_callback,  reduce_lr, early_stopping, tensorboard_callback],\n",
        "          epochs=n_epochs)\n",
        "#Reload best model\n",
        "solar_model.load_weights(solar_brain_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5WnURx7d6mr",
        "colab_type": "text"
      },
      "source": [
        "###Visualise Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LXUg4hsADbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Tensorboard: A lovely way to visualise loss and accuracy across training epochs, \n",
        "as well as cool stuff like an in-depth model architecture diagram.\n",
        "(Colab Only)\"\"\"\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=\"/content/tensor_logs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPcdMO07IdlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Visualise training history:\"\"\"\n",
        "# #Plot Losses\n",
        "fig_acc = go.Figure()\n",
        "fig_acc.add_trace(go.Scatter(\n",
        "                x=list(range(n_epochs)),\n",
        "                y=history.history['loss'],\n",
        "                name='Train Loss',\n",
        "                line_color='red',\n",
        "                opacity=0.8))\n",
        "\n",
        "fig_acc.add_trace(go.Scatter(\n",
        "                x=list(range(n_epochs)),\n",
        "                y=history.history['val_loss'],\n",
        "                name='Test Loss',\n",
        "                line_color='green',\n",
        "                opacity=0.8))\n",
        "#Make it fancy!\n",
        "fig_acc.update_layout(go.Layout(\n",
        "    title=dict(x=0.45),\n",
        "    yaxis=dict(title='Loss'),\n",
        "    xaxis=dict(title='Epoch'),\n",
        "    title_text=\"Training History - Loss\",\n",
        "    hovermode=\"x\",\n",
        "    template=\"plotly_dark\"))\n",
        "fig_acc.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFx26iP-Y1WV",
        "colab_type": "text"
      },
      "source": [
        "###Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3QWeqt3Y4fM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "solar_model.load_weights(solar_brain_dir)\n",
        "#Load validation file\n",
        "validation = np.load(root_dir + \"Validation_Data/0.npy\")\n",
        "\n",
        "#Split\n",
        "val_X = validation[:,:-1]\n",
        "val_Y = validation[:,-1]\n",
        "\n",
        "#Scale all features by their column's mean and std, then between range [0-1]\n",
        "Y_magic_scaler = MagicScaler()\n",
        "Y_magic_scaler.fit(val_X)\n",
        "\n",
        "std_scaler = StandardScaler(with_mean=False, with_std=False)\n",
        "minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "val_X = minmax_scaler.fit_transform(std_scaler.fit_transform(val_X))\n",
        "\n",
        "batch_size = 14\n",
        "#Split Train into Batches\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(batch_size, val_X.shape[0]):# - diff\n",
        "    X.append(val_X[i-batch_size:i, :n_features])\n",
        "    Y.append(val_Y[i])\n",
        "val_X_batches, val_Y_batches = np.array(X), np.array(Y)\n",
        "\n",
        "raw_predictions = solar_model.predict(val_X_batches)\n",
        "\n",
        "#Unscale to real values\n",
        "predictions = Y_magic_scaler.inverse_transform(raw_predictions)\n",
        "\n",
        "#Plot Predictions\n",
        "fig_1 = go.Figure()\n",
        "\n",
        "fig_1.add_trace(go.Scatter(\n",
        "                x=list(range(len(val_Y_batches))),\n",
        "                y=list(val_Y_batches.ravel()),\n",
        "                name='Actual',\n",
        "                line_color='green',\n",
        "                opacity=0.8))\n",
        "\n",
        "fig_1.add_trace(go.Scatter(\n",
        "                x=list(range(len(val_Y_batches))),\n",
        "                y=list(predictions.ravel()),\n",
        "                name='Predicted',\n",
        "                line_color='red',\n",
        "                opacity=0.8))\n",
        "\n",
        "#Make it fancy!\n",
        "fig_1.update_layout(go.Layout(\n",
        "    title=dict(x=0.45),\n",
        "    yaxis=dict(title='Energy (KWh)'),\n",
        "    xaxis=dict(title='Timestep'),\n",
        "    title_text=\"Predictions vs. Actual\",\n",
        "    hovermode=\"x\",\n",
        "    template=\"plotly_dark\"))\n",
        "\n",
        "fig_1.show()\n",
        "print(predictions.mean(),val_Y.mean())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}